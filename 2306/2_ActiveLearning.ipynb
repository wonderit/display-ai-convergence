{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQzSMgFP84s-"
   },
   "source": [
    "Active Learning Procedure in PyTorch.\n",
    "\n",
    "Reference: [Yoo et al. 2019] Learning Loss for Active Learning (https://arxiv.org/abs/1905.03677) '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n",
      "fatal: destination path 'display-ai-convergence' already exists and is not an empty directory.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "/content/display-ai-convergence/2306\n",
      "1_PIML.ipynb\t\tcifar10    data    models\n",
      "2_ActiveLearning.ipynb\tconfig.py  images  __pycache__\n"
     ]
    }
   ],
   "source": [
    "#Initial Setting\n",
    "!pwd\n",
    "!git clone https://github.com/wonderit/display-ai-convergence.git   # 다운 받을 깃허브 clone 링크\n",
    "!git pull  # 저장소에서 변경 사항을 가져오기 위한\n",
    "%cd ./display-ai-convergence/2306\n",
    "!ls"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-14T17:04:02.071798Z",
     "end_time": "2023-06-14T17:04:08.632777Z"
    },
    "id": "gwU1vH8784s_",
    "outputId": "d6129318-2bed-46bb-fdd4-4919d65a9300",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Install Neptune"
   ],
   "metadata": {
    "collapsed": false,
    "id": "VRgfCyUR84tA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-14T17:04:08.635663Z",
     "end_time": "2023-06-14T17:04:17.814398Z"
    },
    "id": "j67g8G5284tB",
    "outputId": "9f1a86f1-2ed4-4bd3-e00b-83adfa3386a3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: neptune in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.1.31)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.4.0)\n",
      "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune) (2.7.0)\n",
      "Requirement already satisfied: boto3>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.26.153)\n",
      "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (11.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (0.18.3)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (23.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (1.5.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.0.3)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.26.15)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.5.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.153 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.16.0->neptune) (1.29.153)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.16.0->neptune) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.16.0->neptune) (0.6.1)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (5.17.1)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.19.1)\n",
      "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.5.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.4)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2022.7.1)\n",
      "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.19.3)\n",
      "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2.3)\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.1.4)\n",
      "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.8)\n",
      "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.2.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U neptune numpy tqdm\n",
    "# !pip install -q neptun numpy==1.19.2 tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Import Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "id": "yTrEzXFu84tB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-14T17:04:17.819832Z",
     "end_time": "2023-06-14T17:04:17.827040Z"
    },
    "id": "rP1NnZhj84tB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7402a648-3124-41b9-dd46-e2ac90d90031"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-42f5a298e211>:28: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  import neptune.new as neptune\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Torchvison\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom\n",
    "import models.resnet as resnet\n",
    "import models.lossnet as lossnet\n",
    "from config import *\n",
    "from data.sampler import SubsetSequentialSampler\n",
    "\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Create Neptune Run"
   ],
   "metadata": {
    "collapsed": false,
    "id": "RvPNPoT-84tC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-abee947313f2>:2: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  run = neptune.init_run(project='wonderit/ll4al',\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://app.neptune.ai/wonderit/ll4al/e/LLAL-82\n"
     ]
    }
   ],
   "source": [
    "IS_TEST = False\n",
    "run = neptune.init_run(project='wonderit/ll4al',\n",
    "                   tags='cifar10',api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2ZmY3ZjczOC0wYWM2LTQzZGItOTNkZi02Y2Y3ZjkxMDZhZTgifQ==')"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-14T17:05:48.669519Z",
     "end_time": "2023-06-14T17:05:49.714778Z"
    },
    "id": "wc5W9af884tC",
    "outputId": "1550e4fd-091c-41fa-b48b-759b4198db92",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-358c2f67c66e>:29: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  run['config/hyperparameters'] = PARAMS\n",
      "<ipython-input-5-358c2f67c66e>:29: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'torch.device'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  run['config/hyperparameters'] = PARAMS\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "PARAMS = {\n",
    "    'num_train': 10000,\n",
    "    'num_val': 0,\n",
    "    'batch_size': 128,\n",
    "    'subset_size': 10000,\n",
    "    'k': 200,\n",
    "    'margin': 1.0,\n",
    "    'lpl_lambda': 1.0,\n",
    "    'trials': 3,\n",
    "    'cycles': 10,\n",
    "    'epoch': 100,\n",
    "    'lr': 0.1,\n",
    "    'milestones': [80],\n",
    "    'epoch_l': 60,\n",
    "    'sgd_momentum': 0.9,\n",
    "    'weight_decay': 5e-4,\n",
    "    'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'kd_type': 'soft_target',\n",
    "    'is_kd': False,\n",
    "    'T': 4,\n",
    "    'kd_lambda': 0.1,\n",
    "    'beta': 1.0,\n",
    "    're-init-backbone': False,\n",
    "    're-init-module': False,\n",
    "    'is_tbr': False,\n",
    "    'tbr_lambda': 0.5,\n",
    "}\n",
    "run['config/hyperparameters'] = PARAMS"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-14T17:04:18.944853Z",
     "end_time": "2023-06-14T17:04:18.967668Z"
    },
    "id": "LL3P0S2K84tC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "92f4bd80-8c91-44bd-b525-493956f88bcf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Seed\n",
    "random_seed = 425\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "##\n",
    "# Data\n",
    "data_dir = 'data/CIFAR10'\n",
    "data_tfms = {\n",
    "    'train':\n",
    "            T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomCrop(size=32, padding=4),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "        ]),\n",
    "    'test':\n",
    "            T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) # T.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # CIFAR-100\n",
    "        ])\n",
    "\n",
    "}\n",
    "cifar10_train = CIFAR10(data_dir, train=True, download=True, transform=data_tfms['train'])\n",
    "cifar10_unlabeled   = CIFAR10(data_dir, train=True, download=True, transform=data_tfms['test'])\n",
    "cifar10_test  = CIFAR10(data_dir, train=False, download=True, transform=data_tfms['test'])\n",
    "dataset_size = {'train': len(cifar10_train), 'test': len(cifar10_test)}"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-14T17:04:18.953726Z",
     "end_time": "2023-06-14T17:04:58.246887Z"
    },
    "id": "dYhoOq1A84tD",
    "outputId": "182a053a-c486-41ce-a10d-787bdf128363",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-7-bef7137d3b09>:2: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'torchvision.transforms.transforms.Compose'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n",
      "  run[\"config/dataset/transforms\"] = data_tfms\n"
     ]
    }
   ],
   "source": [
    "run[\"config/dataset/path\"] = data_dir\n",
    "run[\"config/dataset/transforms\"] = data_tfms\n",
    "run[\"config/dataset/size\"] = dataset_size"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-14T17:04:58.248769Z",
     "end_time": "2023-06-14T17:04:58.254304Z"
    },
    "id": "rvDqhXFK84tD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f7101391-f2c9-46f7-a647-e449117f131a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 1/10 || Label set size 200: Test acc 23.93\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 2/10 || Label set size 400: Test acc 23.73\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 3/10 || Label set size 600: Test acc 34.61\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 4/10 || Label set size 800: Test acc 37.64\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 5/10 || Label set size 1000: Test acc 42.17\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 6/10 || Label set size 1200: Test acc 46.45\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 7/10 || Label set size 1400: Test acc 48.85\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 8/10 || Label set size 1600: Test acc 52.09\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 9/10 || Label set size 1800: Test acc 54.75\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 1/3 || Cycle 10/10 || Label set size 2000: Test acc 59.26\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 1/10 || Label set size 200: Test acc 24.24\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 2/10 || Label set size 400: Test acc 27.69\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 3/10 || Label set size 600: Test acc 31.98\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 4/10 || Label set size 800: Test acc 36.19\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 5/10 || Label set size 1000: Test acc 38.16\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 6/10 || Label set size 1200: Test acc 43.42\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 7/10 || Label set size 1400: Test acc 47.5\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 8/10 || Label set size 1600: Test acc 51.72\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 9/10 || Label set size 1800: Test acc 54.48\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 2/3 || Cycle 10/10 || Label set size 2000: Test acc 59.94\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 1/10 || Label set size 200: Test acc 24.96\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 2/10 || Label set size 400: Test acc 29.57\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 3/10 || Label set size 600: Test acc 37.16\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 4/10 || Label set size 800: Test acc 40.64\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 5/10 || Label set size 1000: Test acc 43.63\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 6/10 || Label set size 1200: Test acc 46.9\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 7/10 || Label set size 1400: Test acc 50.59\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 8/10 || Label set size 1600: Test acc 52.91\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 9/10 || Label set size 1800: Test acc 53.02\n",
      ">> Train a Model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Finished.\n",
      "Trial 3/3 || Cycle 10/10 || Label set size 2000: Test acc 59.34\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join('./cifar10', 'train', 'weights')\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "##\n",
    "# Loss Prediction Loss\n",
    "def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n",
    "    assert len(input) % 2 == 0, 'the batch size is not even.'\n",
    "    assert input.shape == input.flip(0).shape\n",
    "\n",
    "    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n",
    "        loss = loss / input.size(0) # Note that the size of input is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * input, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def SoftTarget(out_s, out_t):\n",
    "    loss = F.kl_div(F.log_softmax(out_s/PARAMS['T'], dim=1),\n",
    "                    F.softmax(out_t/PARAMS['T'], dim=1)\n",
    "                    ,reduction='batchmean') * PARAMS['T'] * PARAMS['T']\n",
    "    return loss\n",
    "\n",
    "def TeacherBoundedLoss(out_s, out_t, labels):\n",
    "    mse_t = (out_t - labels) ** 2\n",
    "    mse_s = (out_s - labels) ** 2\n",
    "    flag = (mse_s - mse_t) > 0\n",
    "    loss = (flag * mse_s).mean()\n",
    "    return loss\n",
    "\n",
    "##\n",
    "# Train Utils\n",
    "iters = 0\n",
    "\n",
    "#\n",
    "def train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, cycle, trial):\n",
    "    models['backbone'].train()\n",
    "    models['module'].train()\n",
    "\n",
    "    # load teacher model\n",
    "    if PARAMS['is_kd'] and cycle > 0:\n",
    "        prev_cycle = cycle - 1\n",
    "        teacher_model_path =f'{checkpoint_dir}/teacher_model_cycle{prev_cycle}.pth'\n",
    "        models['teacher_backbone'] = resnet.ResNet18(num_classes=10)\n",
    "        checkpoint = torch.load(teacher_model_path)\n",
    "        models['teacher_backbone'].load_state_dict(checkpoint['state_dict_backbone'])\n",
    "        models['teacher_backbone'].to(PARAMS['device'])\n",
    "        models['teacher_backbone'].eval()\n",
    "        models['teacher_backbone'].train(mode=False)\n",
    "        models['have_teacher'] = True\n",
    "        models['teacher_module'] = lossnet.LossNet()\n",
    "        models['teacher_module'].load_state_dict(checkpoint['state_dict_module'])\n",
    "        models['teacher_module'].to(PARAMS['device'])\n",
    "        models['teacher_module'].eval()\n",
    "        models['teacher_module'].train(mode=False)\n",
    "\n",
    "    global iters\n",
    "\n",
    "    for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n",
    "        inputs = data[0].to(PARAMS['device'])\n",
    "        labels = data[1].to(PARAMS['device'])\n",
    "        iters += 1\n",
    "\n",
    "        optimizers['backbone'].zero_grad()\n",
    "        optimizers['module'].zero_grad()\n",
    "\n",
    "        scores, features = models['backbone'](inputs)\n",
    "        target_loss = criterion(scores, labels)\n",
    "\n",
    "        if epoch > epoch_loss:\n",
    "            # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model.\n",
    "            features[0] = features[0].detach()\n",
    "            features[1] = features[1].detach()\n",
    "            features[2] = features[2].detach()\n",
    "            features[3] = features[3].detach()\n",
    "        pred_loss = models['module'](features)\n",
    "        pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "        m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)\n",
    "        m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=PARAMS['margin'])\n",
    "        loss            = m_backbone_loss + PARAMS['lpl_lambda'] * m_module_loss\n",
    "\n",
    "        if models.get('have_teacher', False):\n",
    "            teacher_outputs, teacher_feature = models['teacher_backbone'](inputs)\n",
    "            teacher_pred_loss = models['teacher_module'](teacher_feature)\n",
    "            teacher_pred_loss = teacher_pred_loss.view(teacher_pred_loss.size(0))\n",
    "\n",
    "            kd_loss = SoftTarget(scores, teacher_outputs)\n",
    "            if not IS_TEST:\n",
    "                run[f'train/trial{trial}/cycle{cycle}/batch/kd_loss({PARAMS[\"kd_type\"]})'].log(kd_loss.item())\n",
    "            loss = loss + PARAMS['kd_lambda'] * kd_loss\n",
    "\n",
    "            if PARAMS['is_tbr']:\n",
    "                tbr_loss = TeacherBoundedLoss(pred_loss, teacher_pred_loss, target_loss)\n",
    "                loss = loss + PARAMS['tbr_lambda'] * tbr_loss\n",
    "                if not IS_TEST:\n",
    "                    run[f'train/trial{trial}/cycle{cycle}/batch/tbr_loss({PARAMS[\"tbr_lambda\"]})'].log(tbr_loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizers['backbone'].step()\n",
    "        optimizers['module'].step()\n",
    "        if not IS_TEST:\n",
    "            run[f'train/trial{trial}/cycle{cycle}/batch/backbone_loss'].log(m_backbone_loss.item())\n",
    "            run[f'train/trial{trial}/cycle{cycle}/batch/module_loss'].log(m_module_loss.item())\n",
    "            run[f'train/trial{trial}/cycle{cycle}/batch/total_loss'].log(loss.item())\n",
    "\n",
    "\n",
    "#\n",
    "def test(models, dataloaders, mode='val'):\n",
    "    assert mode == 'val' or mode == 'test'\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in dataloaders[mode]:\n",
    "            inputs = inputs.to(PARAMS['device'])\n",
    "            labels = labels.to(PARAMS['device'])\n",
    "\n",
    "            scores, _ = models['backbone'](inputs)\n",
    "            _, preds = torch.max(scores.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "#\n",
    "def train(models, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss, cycle_number, trial_number):\n",
    "    print('>> Train a Model.')\n",
    "    best_acc = 0.\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_epoch(models, criterion, optimizers, dataloaders, epoch, epoch_loss, cycle_number, trial_number)\n",
    "\n",
    "        schedulers['backbone'].step()\n",
    "        schedulers['module'].step()\n",
    "        # if PARAMS['is_kd'] and epoch % 5 == 4:\n",
    "        #     acc = test(models, dataloaders, 'test')\n",
    "        #     if best_acc < acc:\n",
    "        #         best_acc = acc\n",
    "        #         torch.save({\n",
    "        #             'epoch': epoch + 1,\n",
    "        #             'state_dict_backbone': models['backbone'].state_dict(),\n",
    "        #             'state_dict_module': models['module'].state_dict()\n",
    "        #         },\n",
    "        #         f'{checkpoint_dir}/teacher_model_cycle{cycle_number}.pth')\n",
    "        #     print('Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc))\n",
    "\n",
    "    print('>> Finished.')\n",
    "\n",
    "    # Save a checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict_backbone': models['backbone'].state_dict(),\n",
    "        'state_dict_module': models['module'].state_dict()\n",
    "    },\n",
    "        f'{checkpoint_dir}/teacher_model_cycle{cycle_number}.pth')\n",
    "\n",
    "#\n",
    "def get_uncertainty(models, unlabeled_loader):\n",
    "    models['backbone'].eval()\n",
    "    models['module'].eval()\n",
    "    uncertainty = torch.tensor([]).to(PARAMS['device'])\n",
    "    pseudo_label = torch.tensor([]).to(PARAMS['device'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in unlabeled_loader:\n",
    "            inputs = inputs.to(PARAMS['device'])\n",
    "\n",
    "            scores, features = models['backbone'](inputs)\n",
    "            pred_loss = models['module'](features) # pred_loss = criterion(scores, labels) # ground truth loss\n",
    "            pred_loss = pred_loss.view(pred_loss.size(0))\n",
    "\n",
    "            uncertainty = torch.cat((uncertainty, pred_loss), 0)\n",
    "\n",
    "            _, preds = torch.max(scores.data, 1)\n",
    "            pseudo_label = torch.cat((pseudo_label, preds), 0)\n",
    "\n",
    "    return uncertainty.cpu(), pseudo_label.cpu()\n",
    "\n",
    "vis = None\n",
    "##\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    # vis = visdom.Visdom(server='http://localhost', port=9000)\n",
    "    # plot_data = {'X': [], 'Y': [], 'legend': ['Backbone Loss', 'Module Loss', 'Total Loss']}\n",
    "\n",
    "    for trial in range(PARAMS['trials']):\n",
    "        # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "        indices = list(range(PARAMS['num_train']))\n",
    "        random.shuffle(indices)\n",
    "        labeled_set = indices[:PARAMS['k']]\n",
    "        unlabeled_set = indices[PARAMS['k']:]\n",
    "\n",
    "        train_loader = DataLoader(cifar10_train, batch_size=PARAMS['batch_size'],\n",
    "                                  sampler=SubsetRandomSampler(labeled_set),\n",
    "                                  pin_memory=True)\n",
    "        test_loader  = DataLoader(cifar10_test, batch_size=PARAMS['batch_size'])\n",
    "        dataloaders  = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "        # Model\n",
    "        resnet18    = resnet.ResNet18(num_classes=10).to(PARAMS['device'])\n",
    "        loss_module = lossnet.LossNet().to(PARAMS['device'])\n",
    "        models      = {'backbone': resnet18, 'module': loss_module}\n",
    "\n",
    "        # Add Teacher for KD\n",
    "        if PARAMS['is_kd']:\n",
    "            models['teacher_backbone'] = None\n",
    "            models['teacher_module'] = None\n",
    "            models['have_teacher'] = False\n",
    "\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Active learning cycles\n",
    "        for cycle in range(PARAMS['cycles']):\n",
    "            # Re init model\n",
    "            if PARAMS['re-init-backbone'] and cycle > 0:\n",
    "                resnet18    = resnet.ResNet18(num_classes=10).to(PARAMS['device'])\n",
    "                models['backbone'] = resnet18\n",
    "\n",
    "            if PARAMS['re-init-module'] and cycle > 0:\n",
    "                loss_module = lossnet.LossNet().to(PARAMS['device'])\n",
    "                models['module'] = loss_module\n",
    "\n",
    "            # Loss, criterion and scheduler (re)initialization\n",
    "            criterion      = nn.CrossEntropyLoss(reduction='none')\n",
    "            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=PARAMS['lr'],\n",
    "                                    momentum=PARAMS['sgd_momentum'], weight_decay=PARAMS['weight_decay'])\n",
    "            optim_module   = optim.SGD(models['module'].parameters(), lr=PARAMS['lr'],\n",
    "                                    momentum=PARAMS['sgd_momentum'], weight_decay=PARAMS['weight_decay'])\n",
    "            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=PARAMS['milestones'])\n",
    "            sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=PARAMS['milestones'])\n",
    "\n",
    "            optimizers = {'backbone': optim_backbone, 'module': optim_module}\n",
    "            schedulers = {'backbone': sched_backbone, 'module': sched_module}\n",
    "\n",
    "            # Training and test\n",
    "            train(models, criterion, optimizers, schedulers, dataloaders, PARAMS['epoch'], PARAMS['epoch_l'], cycle, trial)\n",
    "            acc = test(models, dataloaders, mode='test')\n",
    "            print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial+1, PARAMS['trials'], cycle+1, PARAMS['cycles'], len(labeled_set), acc))\n",
    "\n",
    "            # Log acc\n",
    "            if not IS_TEST:\n",
    "                run[f'test/trial_{trial}/acc'].log(acc)\n",
    "            ##\n",
    "            #  Update the labeled dataset via loss prediction-based uncertainty measurement\n",
    "\n",
    "            # Randomly sample 10000 unlabeled data points\n",
    "            random.shuffle(unlabeled_set)\n",
    "            subset = unlabeled_set[:PARAMS['subset_size']]\n",
    "\n",
    "            # Create unlabeled dataloader for the unlabeled subset\n",
    "            unlabeled_loader = DataLoader(cifar10_unlabeled, batch_size=PARAMS['batch_size'],\n",
    "                                          sampler=SubsetSequentialSampler(subset), # more convenient if we maintain the order of subset\n",
    "                                          pin_memory=True)\n",
    "\n",
    "            # Measure uncertainty of each data points in the subset\n",
    "            uncertainty, pseudo_label = get_uncertainty(models, unlabeled_loader)\n",
    "\n",
    "\n",
    "            # Index in ascending order\n",
    "            arg = np.argsort(uncertainty)\n",
    "\n",
    "            # Update the labeled dataset and the unlabeled dataset, respectively\n",
    "            labeled_set += list(torch.tensor(subset)[arg][-PARAMS['k']:].numpy())\n",
    "            unlabeled_set = list(torch.tensor(subset)[arg][:-PARAMS['k']].numpy()) + unlabeled_set[PARAMS['subset_size']:]\n",
    "\n",
    "            # Create a new dataloader for the updated labeled dataset\n",
    "            dataloaders['train'] = DataLoader(cifar10_train, batch_size=PARAMS['batch_size'],\n",
    "                                              sampler=SubsetRandomSampler(labeled_set),\n",
    "                                              pin_memory=True)\n",
    "\n",
    "        # Save a checkpoint\n",
    "        torch.save({\n",
    "                    'trial': trial + 1,\n",
    "                    'state_dict_backbone': models['backbone'].state_dict(),\n",
    "                    'state_dict_module': models['module'].state_dict()\n",
    "                },\n",
    "                './cifar10/train/weights/active_resnet18_cifar10_trial{}.pth'.format(trial))"
   ],
   "metadata": {
    "id": "MGVySmPH84tD",
    "outputId": "8183fe13-0149-4fda-fa15-18b79659bf2d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "run.stop()"
   ],
   "metadata": {
    "id": "p43GJ26rBQmq",
    "outputId": "559391f3-4216-4966-84e8-2e2e73e493e4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/wonderit/ll4al/e/LLAL-82/metadata\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
